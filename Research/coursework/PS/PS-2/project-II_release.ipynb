{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsR-pjMWoQFf"
   },
   "source": [
    "1. Ensure you fill in all cells containing `YOUR CODE HERE`, `YOUR ANSWER HERE`, and `NotImplementedError()`.\n",
    "2. After you finish, `Restart the kernel & run` all cells in order.\n",
    "3. Scores will be awarded based on the code, not based on the higher accuracy the better grade. However, the expected accuracy will need to be > 70%.\n",
    "\n",
    "\n",
    "In this assignment, you will explore the task of text classification using transformer-based models, specifically focusing on classifying spam messages. Text classification is a fundamental problem in natural language processing (NLP) where the goal is to assign predefined labels to text. For this task, you will use a dataset containing text messages labeled as either \"spam\" or \"ham\" (non-spam). Your objective is to build a model that can automatically detect and classify spam messages with high accuracy. To achieve this, you will employ a transformer architecture, which has become a state-of-the-art method for various NLP tasks due to its ability to capture complex relationships within text through attention mechanisms.\n",
    "\n",
    "To help you get started, here are some helpful blogs to review:   \n",
    "[1]  https://jalammar.github.io/illustrated-transformer/     \n",
    "[2]  https://mvschamanth.medium.com/decoder-only-transformer-model-521ce97e47e2     \n",
    "[2]  https://huggingface.co/docs/transformers/en/model_doc/bert    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6g-G0g210GQ",
    "outputId": "195791cb-9f4a-4093-d404-3db3919f50da"
   },
   "source": [
    "# Project II: Text Classification Using Transformer Network\n",
    "## Deadline: Nov 14, 11:59 pm\n",
    "\n",
    "You have learned about the basics of neural network training and testing during the class. Let's proceed to the text classification tasks using simple Transformer  networks!\n",
    "    \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo2DgS6onvfP"
   },
   "source": [
    "# Part 1: Transformer Network (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcqf0h8Fn-OD"
   },
   "source": [
    "**Import library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jK5fQiIeNv7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from fancy_einsum import einsum\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from fancy_einsum import einsum\n",
    "# Load data\n",
    "df = pd.read_csv('sms_spam.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGhRSoP2rBx3"
   },
   "source": [
    "**Data processing**   \n",
    "\n",
    "In this assignment, the text data is preprocessed by first converting each message into tokens using a simple tokenization method, where the text is converted to lowercase and split into individual words. A vocabulary is then built from these tokenized texts, assigning a unique index to each word based on its frequency, with reserved indices for unknown words and padding. Each text message is subsequently encoded into a sequence of numerical indices corresponding to the words in the vocabulary. To ensure uniform input lengths for the transformer model, sequences longer than the specified maximum sentence length are truncated, while shorter sequences are padded with a designated padding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7KwkxeqQg6b0"
   },
   "outputs": [],
   "source": [
    "pad_index = 0\n",
    "unknown_index = 1\n",
    "\n",
    "# Tokenizing\n",
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0aJiHLyYq-T2"
   },
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_texts):\n",
    "    vocab = Counter()\n",
    "    for tokens in tokenized_texts:\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    vocab = {word: i + 2 for i, (word, _) in enumerate(vocab.most_common())}\n",
    "    vocab_size = len(vocab) + 2\n",
    "    return vocab, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oY-LcKzSq-Wn",
    "outputId": "2423b5fe-ce9d-4ea2-a0bd-75029ec97647"
   },
   "outputs": [],
   "source": [
    "texts = df['text'].apply(tokenize).tolist()\n",
    "vocab, vocab_size = build_vocab(texts)\n",
    "# print(vocab)\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B7nS2r5-q-ZN"
   },
   "outputs": [],
   "source": [
    "# Convert tokens to integers, if token is not in vocab, assign unknown_index\n",
    "def encode(tokens):\n",
    "    return [vocab.get(token, unknown_index) for token in tokens]\n",
    "\n",
    "encoded_texts = [encode(tokens) for tokens in texts]\n",
    "Max_sentence_length=50\n",
    "for sample_i in range(len(encoded_texts)):\n",
    "    if len(encoded_texts[sample_i])>Max_sentence_length:\n",
    "        encoded_texts[sample_i]=encoded_texts[sample_i][:Max_sentence_length]\n",
    "# Convert labels to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df['type']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iI9ksK2_g_GG"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.texts[idx]), torch.LongTensor([self.labels[idx]])\n",
    "\n",
    "# Padding function\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    text_lengths = [len(text) for text in texts]\n",
    "    texts = pad_sequence(texts, padding_value=pad_index, batch_first=True)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return texts, labels, text_lengths\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XGvzluWS_rsI"
   },
   "source": [
    "**Question 1 (15 points):** Define network architecture\n",
    "\n",
    "There is a good discussion about the transformer: https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work     \n",
    "In this task, you will implement the Transformer network using the PyTorch library.     \n",
    "First, you need to implement the multi-head attention block (MHA). (5 points)  \n",
    "Then, you will make a decoder layer consisting of a MHA and feedforward layer. (5 points)     \n",
    "Last, you will build a transformer network with multiple decoder layers.  (5 points)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The structure of MHA.   \n",
    "![alt text](images/mha.png)\n",
    "\n",
    "A multi-head attention block consists of four consecutive stages:\n",
    "\n",
    "Linear Transformations: The first stage involves three linear (dense) layers that process the input queries, keys, and values.\n",
    "\n",
    "Scaled Dot-Product Attention: In the second stage, a scaled dot-product attention function is applied. This process is repeated h times in parallel, where h refers to the number of heads in the multi-head attention block.\n",
    "\n",
    "Concatenation: The third stage concatenates the outputs from the different attention heads.\n",
    "\n",
    "Final Linear Layer: The final stage applies a linear (dense) layer to produce the overall output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xzhAhtwyhA2a"
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, d_model, n_heads, max_len) -> None:\n",
    "        \"\"\"Initialize the MultiHeadAttention module.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # Set number of heads\n",
    "        d_head: int = int(d_model / n_heads)\n",
    "\n",
    "        # Store d_head sqrt for the attention calculation\n",
    "        self.d_head_sqrt: float = math.sqrt(d_head)\n",
    "\n",
    "        # Create the parameters\n",
    "        self.weight_query = \n",
    "        self.weight_key = \n",
    "        self.weight_value = \n",
    "        self.weight_out = \n",
    "\n",
    "        # Initialise the weights\n",
    "        # Use Kaiming for the QKV weights as we have non-linear functions after them. Use Xavier for\n",
    "        # the output weights as we have no activation function after it.\n",
    "        nn.init.kaiming_normal_(self.weight_query)\n",
    "        nn.init.kaiming_normal_(self.weight_key)\n",
    "        nn.init.kaiming_normal_(self.weight_value)\n",
    "        nn.init.xavier_normal_(self.weight_out)\n",
    "\n",
    "        # Create the minus infinity mask\n",
    "        minus_infinity = torch.full((max_len, max_len), float(\"-inf\"))\n",
    "        minus_infinity_triangle = torch.triu(minus_infinity, diagonal=1)\n",
    "        self.register_buffer(\"minus_infinity_triangle\", minus_infinity_triangle)\n",
    "\n",
    "    def mask(self, attention_pattern):\n",
    "        \n",
    "        n_tokens: int = attention_pattern.shape[-1]\n",
    "        return attention_pattern + self.minus_infinity_triangle[:n_tokens, :n_tokens]\n",
    "\n",
    "    def attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "    ):\n",
    "        # Calculate the numerator\n",
    "        key_transpose = rearrange(\n",
    "            key,\n",
    "            \"batch head pos d_head -> batch head d_head pos\",\n",
    "        )\n",
    "        numerator = query @ key_transpose\n",
    "\n",
    "        # Apply softmax over the attention pattern\n",
    "        attention_pattern = numerator / self.d_head_sqrt\n",
    "        masked_attention = self.mask(attention_pattern)\n",
    "        softmax_part = \n",
    "\n",
    "        return \n",
    "\n",
    "    def forward(self, residual_stream):\n",
    "        # Create the query, key and value\n",
    "        query = \n",
    "        key = \n",
    "        value = \n",
    "\n",
    "        # Get the attention & concat\n",
    "        attn = \n",
    "        attn_concat = \n",
    "\n",
    "        # Multiply by W_O\n",
    "        multi_head_out = \n",
    "\n",
    "        # Return the attention output\n",
    "        return multi_head_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the structure of decoder layer:  \n",
    "![alt text](images/decoder_layer.png)\n",
    "\n",
    "A Decoder Layer has two main parts:\n",
    "\n",
    "Attention Step: This part helps tokens (words) communicate with each other.\n",
    "\n",
    "Feed Forward Step: This part is where the predicted tokens are computed.\n",
    "\n",
    "Around both steps are residual (skip) connections, shown as plus signs in the diagram. These connections let data either go through the layers or skip them. This makes it easier for the model to choose how the data flows.\n",
    "\n",
    "The positionwise feedforward layer is a two-layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"positionwise feedforward layer\n",
    "\n",
    "    The MLP module takes an input of the residual stream and applies a standard two-layer\n",
    "    feed forward network. The resulting output will then be added back onto the residual stream by\n",
    "    the transformer.\n",
    "\n",
    "    MLP(x) = max(0, xW1 + b1)W2 + b2\n",
    "\n",
    "    https://arxiv.org/pdf/1706.03762.pdf (p5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden) -> None:\n",
    "        \"\"\"MLP Sub-Layer Initialisation.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight_inner= \n",
    "\n",
    "        self.bias_inner = \n",
    "\n",
    "        self.weight_outer = \n",
    "\n",
    "        self.bias_outer = \n",
    "\n",
    "        # Initialise the weights\n",
    "        # We use Kaiming Initialization for the inner weights, as we have a non-symmetric activation\n",
    "        # function (ReLU)\n",
    "        nn.init.kaiming_normal_(self.weight_inner)\n",
    "\n",
    "        # We use Xavier Initialization for the outer weights, as we have no activation function\n",
    "        nn.init.xavier_normal_(self.weight_outer)\n",
    "\n",
    "    def forward(self, residual_stream):\n",
    "        \"\"\"Forward Pass through the MLP Sub-Layer.\n",
    "\n",
    "        Args:\n",
    "            residual_stream (ResidualStream): MLP input\n",
    "\n",
    "        Returns:\n",
    "            ResidualStream: MLP output\n",
    "        \"\"\"\n",
    "        # Inner = relu(x W1 + b1)\n",
    "        \n",
    "\n",
    "        # Outer = inner @ W2 + b2\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, max_len):\n",
    "        \"\"\"Initialise the full layer.\"\"\"\n",
    "        super(DecoderOnlyLayer, self).__init__()\n",
    "\n",
    "        # Create the feed forward and attention sub-layers\n",
    "        self.feed_forward = \n",
    "        self.layer_norm_ff = \n",
    "        self.attention = \n",
    "        self.layer_norm_attn = \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, residual_stream):\n",
    "        # Attention\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use these modules to build your transformer.  \n",
    "![alt text](images/decoder_only_model.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(torch.nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, d_model, max_len) -> None:\n",
    "        \"\"\"Initialize the positional encoding matrix.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create everything inside the parentheses\n",
    "        # inner = pos/(10000^(2i/d_model) = pos/wavelength\n",
    "        positions = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        dimensions_2 = torch.arange(0, d_model, 2).float()\n",
    "        inner = positions / (10000 ** (dimensions_2 / d_model))\n",
    "\n",
    "        # Create interweaved positional encoding\n",
    "        pos_encoding = torch.zeros(max_len, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(inner)\n",
    "        pos_encoding[:, 1::2] = torch.cos(inner)\n",
    "\n",
    "        # Register as a non-persistent buffer so that it isn't stored in the state dict. This is\n",
    "        # important as it allows the transformer to be instantiated with a different `max_tokens`\n",
    "        # value, whilst still re-using the same state dict.\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding, persistent=False)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        \"\"\"Apply the positional encoding to the given input embedding.\n",
    "\n",
    "        Args:\n",
    "            embedding (ResidualStream): The input embedding with shape (batch_size, tokens,\n",
    "                d_model).\n",
    "\n",
    "        Returns:\n",
    "            ResidualStream: The output embedding with positional encoding applied, having the same\n",
    "                shape as the input embedding (batch_size, tokens, d_model).\n",
    "        \"\"\"\n",
    "        #print(embedding.shape)\n",
    "        num_tokens_in_embedding= embedding.shape[-2]\n",
    "        trimmed_pos_encoding= self.pos_encoding[\n",
    "            :num_tokens_in_embedding,\n",
    "            :,\n",
    "        ]\n",
    "        return trimmed_pos_encoding + embedding\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,  dec_voc_size, d_model, n_head, max_len,ffn_hidden, n_layers, classes):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(dec_voc_size, d_model)\n",
    "        self.cls_head = torch.nn.Linear(d_model, classes) # Unembed(dec_voc_size, d_model,  device)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Layers\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(DecoderOnlyLayer(d_model, ffn_hidden, n_head, max_len))\n",
    "        \n",
    "\n",
    "    def forward(self, src):\n",
    "        residual_stream= self.embed(src)\n",
    "        residual_stream = self.positional_encoding(residual_stream)\n",
    "\n",
    "        # Loop through layers\n",
    "        for layer in self.layers:\n",
    "            residual_stream = layer(residual_stream)\n",
    "\n",
    "        # Unembed and return\n",
    "        return self.cls_head(residual_stream)[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02UaoAZ6AF73"
   },
   "source": [
    "**Question 2 (5 points):** Define training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "965x_R2WhDR8"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Feel free to change these hyper-parameters and optimizers!\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "NUM_HEAD = 2\n",
    "MAX_LEN = Max_sentence_length\n",
    "NUM_LAYERS=2\n",
    "OUTPUT_DIM = len(np.unique(labels))\n",
    "model = Transformer(vocab_size, EMBEDDING_DIM, NUM_HEAD, MAX_LEN, HIDDEN_DIM, MAX_LEN, OUTPUT_DIM).to(device)\n",
    "import tqdm\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0.\n",
    "    n=0\n",
    "    for texts, labels, text_lengths in tqdm.tqdm(loader):\n",
    "        # YOUR CODE HERE\n",
    "        # Define your training logic here\n",
    "        # Convert data to same device with model\n",
    "        \n",
    "    return epoch_loss / len(loader)\n",
    "# train_loss = train(model, train_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CogxGP1zAoXC"
   },
   "source": [
    "**Question 3 (5 points):** Define eval logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tmeA6-K7vaxF"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels, text_lengths in tqdm.tqdm(loader):\n",
    "            \n",
    "    return epoch_loss / len(loader), accuracy\n",
    "# evaluate(model,test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuTF98Xzvay4"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, _ = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"\\tTest Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms9SfJAiiryn"
   },
   "outputs": [],
   "source": [
    "_, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\tTest Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvjqc0DFnzdG"
   },
   "source": [
    "# Part 2: Fine-tune the Pre-trained Transformer (15 points)\n",
    "\n",
    "Useful resource: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwDHUTJ4oQFg"
   },
   "source": [
    "Import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T6usoztWoQFh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMoW74vk18x_"
   },
   "source": [
    "Load the data from the csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKrbKtV7oQFi"
   },
   "source": [
    "The loaded files shown in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGxmqxNJ2M_x",
    "outputId": "41580315-b1f8-4c20-97dc-439b25b993c1"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    labels = df[\"type\"].unique().tolist()\n",
    "    label_dict = {label: i for i, label in enumerate(labels)}\n",
    "    df['label'] = df[\"type\"].replace(label_dict)\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in df[\"text\"]:\n",
    "        encoded_dict = tokenizer.encode_plus(text, add_special_tokens=True, max_length=64, pad_to_max_length=True, return_attention_mask=True)\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(df['label'].values), label_dict\n",
    "df=pd.read_csv('sms_spam.csv')\n",
    "input_ids, attention_masks, labels, label_dict = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nSvYVI4t2fmC"
   },
   "outputs": [],
   "source": [
    "def split_dataset(input_ids, attention_masks, labels):\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    return random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataset, test_dataset = split_dataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UUP44T82sRf",
    "outputId": "af79cbfb-1b01-4998-d167-e08d8dd3182b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def create_model(label_dict):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict))\n",
    "    return model\n",
    "\n",
    "model = create_model(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNEKgpacoQFl"
   },
   "source": [
    "**Question 4 (5 points):** Please define the optimizer with Adam, AdamW, or SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wQQktipt2zRK"
   },
   "outputs": [],
   "source": [
    "def setup_training(model):\n",
    "    # YOUR CODE HERE\n",
    "    optimizer = \n",
    "\n",
    "    epochs = 1\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "    return optimizer, epochs, scheduler\n",
    "\n",
    "optimizer, epochs, scheduler = setup_training(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brESWGnaoQFl"
   },
   "source": [
    "**Question 5 (5 points):** please define  training strategy with model, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snuh3_BE2714"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, optimizer, scheduler, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            b_input_ids = batch[0].to('cuda')\n",
    "            b_attention_mask = batch[1].to('cuda')\n",
    "            b_labels = batch[2].to('cuda')\n",
    "\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            \n",
    "            # Get the loss from model outputs\n",
    "            \n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            \n",
    "\n",
    "            # Clip the gradient to avoid exploding gradients (optional, but recommended)\n",
    "            \n",
    "\n",
    "            # Optimizer step: update model parameters\n",
    "            \n",
    "\n",
    "            # Scheduler step: update the learning rate\n",
    "            \n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('\\r [epoch: %03d][iter: %04d][loss: %.6f]'%(epoch+1, step, loss.item()))\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f'Average Training Loss: {avg_train_loss:.4f}')\n",
    "model.to(device)\n",
    "train_model(model, train_dataloader, optimizer, scheduler, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMCi7BZsoQFm"
   },
   "source": [
    "**Question 6 (5 points):** please define evaluation strategy with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vWMfjdL53KaL"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to('cuda')\n",
    "        b_attention_mask = batch[1].to('cuda')\n",
    "        b_labels = batch[2].to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            \n",
    "\n",
    "            # Get the predicted logits (raw predictions before applying softmax)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    return predictions, true_labels\n",
    "\n",
    "predictions, true_labels = evaluate_model(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwUNHetpoQFm"
   },
   "source": [
    "Then, let us eval the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lO_NDhrB3M_e"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, true_labels):\n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    predicted_label_ids = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    return accuracy_score(flat_true_labels, predicted_label_ids)\n",
    "\n",
    "accuracy = compute_accuracy(predictions, true_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDvAp3uG5m5s"
   },
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "predicted_label_ids = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "report = classification_report(flat_true_labels, predicted_label_ids, target_names=label_dict.keys())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0KGg1gG9nr4"
   },
   "source": [
    "# Part 3: Advanced fine-tuning (Grad student only) (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Og-U9gZCAmu"
   },
   "source": [
    "Sometimes, it is not easy to fine-tune the whole model with limited memory. A easy solution is to only fine tune the last layer.   \n",
    "In this task, you are encouraged to try two strategies:   \n",
    "(1) Last Layer training:   \n",
    "    In Last Layer Training, we freeze all the layers of the model except for the last (output) layer. This helps in reducing memory usage and training time since only a small part of the model is updated during training.\n",
    "(2) LoRA:   https://huggingface.co/docs/peft/main/en/conceptual_guides/lora     \n",
    "    LoRA allows efficient fine-tuning by adding low-rank trainable matrices to the attention layers of a pre-trained transformer model. Instead of updating the full model parameters, LoRA only updates the low-rank matrices, reducing the memory and computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5BF3y8m-BgK"
   },
   "source": [
    "**Question 7 (5 points)**: Only opitmize the last layer for fine-tuning the Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the parameters and define the optimizer\n",
    "def freeze_bert_layers(model):\n",
    "    # Freeze all layers except the last classification layer\n",
    "    \n",
    "\n",
    "# Create the model\n",
    "model = create_model(label_dict)\n",
    "\n",
    "# Freeze layers\n",
    "freeze_bert_layers(model)\n",
    "\n",
    "# Check which layers are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_optimizer(model, learning_rate=5e-5):\n",
    "    # Only update parameters that have requires_grad=True (i.e., not frozen layers)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "# Example call\n",
    "optimizer = create_optimizer(model)\n",
    "NUM_EPOCHS = 1\n",
    "model.to('cuda')\n",
    "train_model(model, train_dataloader, optimizer, scheduler, NUM_EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJaeUAizysjY"
   },
   "outputs": [],
   "source": [
    "predictions, true_labels = evaluate_model(model, test_dataloader)\n",
    "accuracy = compute_accuracy(predictions, true_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4jE7OVRzrPM"
   },
   "source": [
    "**Question 8 (5 points)**:  Applying the LoRA for fine-tuning the Bert     \n",
    "Plese read this repo to learn how to apply the LoRA to your model     \n",
    "https://github.com/fkodom/lora-pytorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lora-pytorch\n",
    "from lora_pytorch import LoRA\n",
    "from transformers.models.bert.modeling_bert import BertAttention \n",
    "import tqdm\n",
    "class BertWithLoRA(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertWithLoRA, self).__init__()\n",
    "        # Load the pretrained BERT model\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict))\n",
    "        \n",
    "        # Apply LoRA to attention layers\n",
    "        \n",
    "        # Freeze all parameters in the model except the LoRA weights and classification layer\n",
    "        \n",
    "        \n",
    "        # LoRA parameters are still trainable\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "model = BertWithLoRA(num_labels=2)\n",
    "\n",
    "# Check which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "train_model(model, train_dataloader, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = evaluate_model(model, test_dataloader)\n",
    "accuracy = compute_accuracy(predictions, true_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
